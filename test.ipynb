{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1) Imports\n",
    "\n",
    "# Selenium library\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "\n",
    "#Processing library\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# IMPORTS AND CUSTOM FUNCTIONS\n",
    "from imports import WebDriver, clean_text, progressBar, send_gmaps_search, click_first_suggestion, collect_pages_result, click_next_page, no_results, get_date, get_duration, get_unique_urls_serie, input_country_kw_cities\n",
    "from variable import Input_city_dict, countries_dict\n",
    "\n",
    "country, keywords_list, locations_list = input_country_kw_cities(Input_city_dict, countries_dict)\n",
    "w_websites = input(\"Only results with website ? (Y/n - Press enter for yes):\")\n",
    "\n",
    "if w_websites in ('', 'Y'):\n",
    "    w_websites = True\n",
    "    unique_websites = input(\"One result per website ? (Y/n - Press enter for yes):\")\n",
    "\n",
    "    if unique_websites in ('', 'Y'):\n",
    "        unique_websites = True\n",
    "        print('Scraping one result per website')\n",
    "\n",
    "    else:\n",
    "        unique_websites = False\n",
    "        print('Scraping all results (dont dedpulicate on website)')\n",
    "\n",
    "else:\n",
    "    w_websites = False\n",
    "    unique_websites = False\n",
    "    print('Scraping all results (with and w/o website)')\n",
    "\n",
    "\n",
    "### COLLECT RESULTS URLs\n",
    "start = time.time()\n",
    "# options = Options()\n",
    "#options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "value = 1\n",
    "endvalue = len(keywords_list)*len(locations_list)\n",
    "nb_url = 0\n",
    "kw_dict = dict()\n",
    "\n",
    "for kw in keywords_list:\n",
    "    \n",
    "    city_dict=dict()\n",
    "    \n",
    "    for loc in locations_list:\n",
    "        results_gmap_url = list()\n",
    "        if country.lower() in loc.lower():\n",
    "            url = \"https://www.google.com/maps/search/\" + clean_text(kw)+\" \"+loc\n",
    "        else : \n",
    "            url = \"https://www.google.com/maps/search/\" + clean_text(kw)+\" \"+loc +\" \"+ country\n",
    "        progressBar(\"gmaps_search_urls...\", value, endvalue, bar_length = 50, width = 20)\n",
    "        value+=1\n",
    "        \n",
    "        try: # Check if search is already scraped\n",
    "            if loc in city_dict.keys():\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # launch gmaps search \n",
    "        send_gmaps_search(url,driver)\n",
    "        # Load variables:\n",
    "        first_suggestion_checked = False   \n",
    "        time.sleep(1)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            try: # Collect current page results\n",
    "                time.sleep(1)\n",
    "                #print(\"start collecting results\")\n",
    "                results_gmap_url += collect_pages_result(driver, w_websites)\n",
    "\n",
    "            except: #redirect to first_suggestion if not already checked and if not \"Aucun r√©sultat\"\n",
    "                #print(\"except while collecting results\")\n",
    "                if not first_suggestion_checked and not no_results(driver):\n",
    "                    # Go to first suggestion and restart collect\n",
    "                    click_first_suggestion(driver)\n",
    "                    first_suggestion_checked = True\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                \n",
    "            \n",
    "            if not click_next_page(driver):\n",
    "                break\n",
    "                       \n",
    "        city_dict[loc] = results_gmap_url.copy()\n",
    "    kw_dict[kw] = city_dict.copy()\n",
    "# driver.quit()\n",
    "\n",
    "end = time.time()\n",
    "for key in kw_dict.keys():\n",
    "    for city in kw_dict[key].keys():\n",
    "        nb_url += len(kw_dict[key][city])\n",
    "unique_urls = len(get_unique_urls_serie(kw_dict))\n",
    "print(\"-\"*30)\n",
    "print(\"Duration : {}\".format(get_duration(start, end)))\n",
    "print(\"Urls scraped : {}\".format(nb_url))\n",
    "print(\"Unique urls scraped : {}\".format(unique_urls))\n",
    "print(\"-\"*30+\"\\n\")\n",
    "\n",
    "### SCRAP INFOS FROM URLs WITH WEBSITES\n",
    "\n",
    "start = time.time()\n",
    "x = WebDriver(driver)\n",
    "result_dict = dict()\n",
    "url_scraped = list()\n",
    "website_scraped = list()\n",
    "count=1\n",
    "total = 0\n",
    "\n",
    "# Calculation of total urls to scrap:\n",
    "for key in kw_dict.keys():\n",
    "    for city in kw_dict[key].keys():\n",
    "        total += len(kw_dict[key][city])\n",
    "\n",
    "row_list = []\n",
    "for kw in kw_dict.keys():\n",
    "    for city in kw_dict[kw].keys():\n",
    "        for url in kw_dict[kw][city]:\n",
    "            progressBar(\"scraping result urls...\", count, total, bar_length = 50, width = 20)\n",
    "            if url not in result_dict.keys():\n",
    "                print(\"url not in result_dict\")\n",
    "                \n",
    "                result = x.scrape(url).copy()\n",
    "                # print(result)\n",
    "                if not unique_websites or result['website'] not in website_scraped:\n",
    "                # if result['website'] not in website_scraped:\n",
    "                    result_dict[url] = {\"url\" : url,\n",
    "                                        \"city\": city,\n",
    "                                        \"kw\" : kw,\n",
    "                                        \"website\" : result['website'],\n",
    "                                        \"category\" : result['category'],\n",
    "                                        \"contact\" : result['contact'],\n",
    "                                        \"location\" : result['location'],\n",
    "                                        \"avg rating\" : result[\"avg rating\"],\n",
    "                                        \"count rating\" : result[\"count rating\"],\n",
    "                                        \"iframe\" : result[\"iframe\"]}\n",
    "                url_scraped.append(url)\n",
    "                website_scraped.append(result['website'])\n",
    "            count+=1\n",
    "\n",
    "x.driver.quit()\n",
    "results_df = pd.DataFrame().from_dict(result_dict, orient = \"index\")\n",
    "\n",
    "if unique_websites:\n",
    "    results_df.drop_duplicates(subset=\"website\", inplace = True)\n",
    "\n",
    "results_df.reset_index(drop = True, inplace = True)\n",
    "                        #   ).drop_duplicates(subset=\"website\"\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "results_df['timestamp'] = ct\n",
    "\n",
    "results_df['date'] = get_date()\n",
    "results_df['duration'] = get_duration(start, end)\n",
    "new_cols = results_df.columns.tolist()[1:]+results_df.columns.tolist()[:1]\n",
    "results_df = results_df[new_cols] ## Move first column to last position\n",
    "\n",
    "result_file_name = f\"results/{country}_{keywords_list[0]}_{datetime.date.strftime(datetime.date.today(),'%Y-%m-%d')}.csv\"\n",
    "results_df.to_csv(result_file_name)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(f\"Duration : {get_duration(start, end)}\")\n",
    "print(f\"Results : {len(results_df)}\")\n",
    "print(f\"Unique websites : {results_df['website'].nunique()}\")\n",
    "print(f\"Results file location : {result_file_name}\")\n",
    "print(\"-\"*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrollbox = driver.find_element(By.CSS_SELECTOR, \"div[role='feed']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_containers = scrollbox.find_elements(by = By.CSS_SELECTOR, value = \"div[class ^= 'Nv2PK']\")\n",
    "search_results = [el.find_element(by = By.CSS_SELECTOR, value = 'a[class = hfpxzc]') for el in search_results_containers]\n",
    "to_keep = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results[0].__dir__()\n",
    "# search_results[0].tag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('retrieve results')\n",
    "for elm in search_results:\n",
    "if not w_websites:\n",
    "    to_keep.append(elm)\n",
    "    continue\n",
    "\n",
    "aria_label = search_results[0].get_attribute('aria-label')\n",
    "css = f\"div[aria-label = '{aria_label}']\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.find_element(By.CSS_SELECTOR, \"div[role='main']\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"div[role='main']\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
